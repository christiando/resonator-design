import torch
import functools
import numpy as np

from typing import Dict, Union
from dataset.data_module import DataModule
from utils import numpy_dict_to_tensor, torch_dict_to_numpy

from torch.utils.data import DataLoader
from pytorch_lightning.callbacks.finetuning import BaseFinetuning


class FineTuningModel:
    """
    Wrapper class for taking a Cond(V)AEModel and fine-tuning the decoder using the encoder as surrogate model.
    It was observed that the decoder may neglect the conditional features y, which is why a second-stage training was suggested,
    where the order of the decoder and encoder are swapped, the encoder frozen and the model subsequently trained to reconstruct the correct y.

    This class copies the complete functionality of the input model, except for the `forward` and `fit` methods, which are redefined for fine-tuning.
    """
    def __init__(self, model:'CondAEModel', gen_z_strategy:str='encode', sample_around_std:float=0.1, **kwargs):
        """
        Parameters:
        model (CondAEModel): Model whose decoder should be fine-tuned using its (frozen) encoder as surrogate.
        gen_z_strategy (str): Which strategy should be employed for optaining the latent vectors z.
            One of 'encode', 'sample' or 'sample_around'. 
            -   if 'encode', the latent variables z are generated by the (frozen) encoder
            -   if 'sample', the latent variables z are sampled normally
            -   if 'sample_around', the latent variables z are generated by the (frozen) encoder and added to random gaussian noise with std `sample_around_std`.
        sample_around_std (float): If `gen_z_strategy`is 'sample_around', the random gaussion noise is sampled using this value as standard deviation.
        """
        self.model = model
        self.gen_z_strategy = gen_z_strategy
        self.sample_around_std = sample_around_std

        # get all functions of `model` and add them to this instance except for `forward` and `fit`, which are redefined.
        for name in dir(model):
            attr = getattr(model, name)
            if callable(attr) and name not in dir(self):
                setattr(self, name, self.__wrap_method(attr))
    

    def __wrap_method(self, method):
        """
        A helper function that wraps the given method and modifies its behavior according to the method name.
        If the method name is `forward`, it returns the result of `self.forward` method.
        If the method name is `fit`, it returns the result of `self.fit` method.
        Otherwise, it returns the result of the given method directly.

        Parameters:
        method (callable): The method to be wrapped.

        Returns:
        wrapper (callable): The wrapped method with modified behavior.
        """
        @functools.wraps(method)
        def wrapper(*args, **kwargs):
            if method.__name__ == 'forward':
                return self.forward(*args, **kwargs)
            elif method.__name__ == 'fit':
                return self.fit(*args, **kwargs)
            else:
                return method(*args, **kwargs)
        return wrapper

    def forward(self, data:Dict[str, Union[np.array, torch.Tensor]], transform:bool=True) -> Union[Dict[str, np.array], Dict[str, torch.Tensor]]:
        """
        Obtains latent vectors z according to `self.gen_z_strategy`, then decodes the data to obtain x_hat
        and finally encodes x_hat using the (frozen) encoder to obtain y_hat.
        """
        if transform and not torch.is_tensor(data[list(data.keys())[0]]):
            data = self.dataset.transform({d_key: data[d_key] for d_key in list(self.encoder.in_heads.keys()) + list(self.decoder.in_heads.keys())})
            torch.set_grad_enabled(False)
            self.eval()

        x = numpy_dict_to_tensor({x_key: data[x_key] for x_key in self.x_heads.keys()}, device=self.device)
        y = numpy_dict_to_tensor({y_key: data[y_key] for y_key in self.y_heads.keys()}, device=self.device)

        if self.gen_z_strategy in ['encode', 'sample_around']:
            if self.pass_y_to_encoder:
                z = self.encoder(torch.cat([x, y], dim=-1))['z']
            else:
                z = self.encoder(x)['z']

            if self.gen_z_strategy == 'sample_around':
                z = z + torch.normal(mean=0., std=self.sample_around_std, size=(y.shape[0], self.latent_dim))

        elif self.gen_z_strategy == 'sample':
            z = torch.normal(mean=0., std=1., size=(y.shape[0], self.latent_dim))

        pred = self.decoder({'z': z, 'y': y})
        if self.pass_y_to_encoder:
             pred.update(self.encoder(x | y))
        else:
             pred.update(self.encoder(pred['x']))

        if transform and not torch.is_tensor(data[list(data.keys())[0]]):
            pred = torch_dict_to_numpy(pred)
            pred['x'] = self.dataset.inverse_transform(pred['x'])
            pred['y'] = self.dataset.inverse_transform(pred['y'])
            torch.set_grad_enabled(True)
            self.train()
        return pred

    def fit(self, data_module:DataModule=None, train_loader:DataLoader=None, val_loader:DataLoader=None, 
            max_epochs:int=100, callbacks:list=None, loggers:list=None, accelerator:str='auto', **kwargs) -> None:
        """
        Launches fine-tuning of the decoder in `self.model`.
        Freezes the encoder prior to calling `self.model.fit()` and unfreezes it upon completion.
        
        Parameters:
            data_module (pl.LightningDataModule): DataModule object that provides the training, validation, and test data. 
            max_epochs (int, optional): The maximum number of epochs to train for. Default: 100.
            callbacks (list, optional): A list of PyTorch Lightning Callback objects to use during training. Default: None.
            loggers (list, optional): A list of PyTorch Lightning Logger objects to use during training. Default: None.
            accelerator (str, optional): Which accelerator should be used (e.g. cpu, gpu, mps, etc.) Default: auto.
        """
        freeze_callback = FreezeEncoder()

        if not callbacks:
            callbacks = []
        callbacks.append(freeze_callback)

        self.model.fit(data_module, train_loader, val_loader, max_epochs, callbacks, loggers, accelerator, **kwargs)

        for optimizer in self.model.trainer.optimizers:
            freeze_callback.unfreeze_and_add_param_group(
                modules=self.model.encoder,
                optimizer=optimizer,
                train_bn=True,
            )


class FreezeEncoder(BaseFinetuning):
    """
    A PyTorch Lightning callback class that extends the `BaseFinetuning` class and 
    freezes the encoder of the `CondAEModel` before training.
    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def freeze_before_training(self, module: 'CondAEModel'):
        """
        Freezes the encoder of the `CondAEModel` before training.

        Parameters:
            module: A `CondAEModel` instance.
        """
        self.freeze(module.encoder, train_bn=False)

    def finetune_function(self, pl_module, current_epoch, optimizer, optimizer_idx):
        pass